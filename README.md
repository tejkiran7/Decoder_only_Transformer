# Decoder-Only Transformer

This repository contains an implementation of a **Decoder-Only Transformer** model from scratch using Python and PyTorch. The model is inspired by architectures like GPT, designed for autoregressive text generation tasks.

## Features

- **Built from scratch** without using high-level transformer libraries  
- Implements **self-attention**, **positional encoding**, **skip connections**, **layer normalization**, and **feed-forward layers**  
- Supports **token embeddings** and **masked self-attention** for autoregressive training  
- Trained on text data for **next-token prediction**  
